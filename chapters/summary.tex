%!TEX root = ../tobias_neumann_phd_thesis.tex

\chapter{Summary and Outlook}

In this thesis, we set out to address the challenges of RNA-seq datasets with elevated nucleotide conversion levels as produced by epitranscriptomics sequencing datasets. To this end, we first investigated the current state-of-the art landscape of reference-based read mapping approaches, their characteristics and suitability for mapping nucleotide conversion containing reads. We found that the territory of suitable epitranscriptome sequencing dataset mapping approaches remains largely uncharted, with the notable exception being bisulfite RNA-sequencing, where the 3N DNA mapping approach was adopted to its RNA counterpart. Given the observation that typically established mapping approaches are transferred to new domains such as epitranscriptome sequencing, we in parallel investigated whether suitable \textit{in silico} simulation frameworks exist that would allow us to benchmark the performance of state-of-the-art tools in the context of nucleotide conversion containing read sets and assess and quantify potential biases and their impact on biological readouts. Similarly to dedicated epitranscriptome sequencing mapping tools, we identified a lack of suitable simulation frameworks that allow for the introduction of nucleotide conversions at given rates as well as the simulation of mixes of (partially) spliced isoforms per transcript to capture the full dynamics of the transcriptional process. \\
To address the need for novel tools dedicated to processing epitranscriptomics sequencing datasets, we presented the Digital Unmasking of Nucleotide conversions in $k$-mers (DUNK) method in chapter \ref{chap:slamdunk}. We implemented DUNK for the SLAM-seq technology in the package SLAM-DUNK and showed that we achieve constant mapping rates independent of nucleotide conversion rates up to 15\% for 100 bp reads, reducing the nucleotide conversion quantification error by $\sim$50\%. Since SLAM-seq employs a 3'end sequencing approach to maximize multiplexing at minimal cost, we furthermore implemented a strategy to recover multi-mapping reads in 3' intervals with low mappability that would otherwise be inaccessible. We could show that we recover an additional 1-7\% of correctly mapped reads, rescuing relevant genes for biological interpretation. To quantify nucleotide conversions robustly and unbiased, we proposed a T-content and read coverage normalized approach to estimate the fraction of labeled transcripts, a central biological readout of SLAM-seq. With our proposed estimate we were able to reduce the proportion of genes with high error rates (relative error $>$20\%) from 23\% down to 8\%. SLAM-DUNK is available on several popular software platforms and runs with a peak memory consumption of under 10 GB and 8 hours with 10 CPU threads for 21 mouse SLAM-seq samples, making it readily available to bench-scientists with heterogeneous desktop compute environments. Adoption of SLAM-DUNK by the scientific community is further encouraged by providing plugins to the popular QC summary tool MultiQC and a release as canonical Nexflow workflow on the nf-core community repository. \\
A main limitation of the broad application of DUNK to the entire landscape of mapping approaches and sequencing technologies is a practical one: In theory it is straightforward to adopt our conversion aware scoring scheme to any type of nucleotide conversion. In practice, most mappers - including NextGenMap we used for SLAM-DUNK - apply the computationally costly alignment calculation for evaluating already preselected putative regions of origin derived from more heuristic approaches such as hash-based searches or compute alignment scores differently. Adopting an approach like BiSS that also enumerates all possible converted states of a $k$-mer during the seed phase might further improve accuracy at the cost of elevated memory consumption and compute time, but we did not adopt it as in initial benchmarks we did not observe any noticeable advantage. Likely, for low conversion rates as inherent to SLAM-seq datasets, tuning only either seed phase search or alignment scoring alone for nucleotide conversions is sufficient. For BWT-based approaches that do not employ classical alignment algorithms in an extend approach, it may prove fruitful to revisit asymmetric backward search approaches that allow for deeper backward search paths containing the nucleotide conversion type expected in a given dataset. From an implementation point of view, often readily available alignment computation libraries are used that are inaccessible to inject customized scoring matrices or use hard-coded scoring matrices, making it very tedious to adopt nucleotide conversion aware scoring schemes as exemplified by our personal communication with the STAR developer we originally prompted to implement this scoring approach. Furthermore, our DUNK implementation SLAM-DUNK was tailored to single-end, unspliced SLAM-seq data as it uses the QuantSeq 3'end sequencing approach and as such is not readily applicable to nowadays more commonly used paired-end sequencing modes or full-length transcript sequencing (including splice junctions). However, we prepared for the adoptability of SLAM-DUNK by designing it modular, allowing for a paired-end parametrization of NextGenMap (the current read mapping engine) or a complete swap of the underlying read mapping engine, given it provides documented BAM tags to allow for efficient nucleotide conversion quantification. Lastly, due to its modular design, SLAM-DUNK also allows to plug-in statistical frameworks like GRAND-SLAM \citep{Juerges2018} as an alternative method for estimating new and old RNA proportions.  \\
To enable researchers to evaluate and benchmark the performance of mapping tools in the domain of sequencing datasets harboring nucleotide conversions, we introduced \textit{splice\_sim} in chapter \ref{chap:splice_sim}. \textit{Splice\_sim} is a novel RNA-seq simulation framework that distinguishes itself from the current landscape of RNA-seq simulators with its ability to simulate nucleotide conversions at arbitrary rates from a mixed model of converted and unconverted transcripts stemming from arbitrary mixes of (partially) spliced isoforms per transcript. Using \textit{splice\_sim}, we generated transcriptome wide mapping accuracies for a range of genomic features of interest for the state-of-the-art mapping tools STAR, HISAT-3N and meRanGs for $>$100,000 annotated mouse and human transcripts. The mappers were selected to cover a variety of mapping approaches and targeted sequencing technologies, with STAR being a general purpose spliced-read mapper, HISAT-3N implementing a generalized 3N alignment strategy and meRanGs representing a dedicated BS-RNA-seq strategy. Complementary, we also simulated datasets with different conversion types and rates and also diverse biological readouts such as the fraction of converted reads to estimate RNA half-lives, the fraction of mature isoform reads to estimate RNA splicing kinetics or the methylation rate to detect post-transcriptional RNA methylation sites. The main conclusion from looking across all simulated datasets and employed mapping tools is that performance is heavily associated with genomic mappability, with highly mappable regions typically displaying excellent performance but impaired mapping performance for medium and low mappable regions, consequently translating into elevated error rates in measures based on the comparison of nucleotide conversion containing read groups. This is important as a considerable faction of these regions contain a substantial number of relevant protein coding and regulatory RNAs, for which wrong estimates of downstream measures have a direct impact on the overall biological interpretation. Through our \textit{splice\_sim} results, we provide comprehensive performance measures that can be used to build gold-standard references. We demonstrate how these can be used for cleaning and flagging data and improving overall accuracy and interpretation by approaches such as filtering problematic introns or selection of the best mapper based on the simulation results per feature in a mosaic approach. While for higher conversion rates, we find indeed the 3N mapping approach to be superior, for lower conversion rates conventional approaches outperform 3N mappers, indicating that reducing the base space to 3 letters drastically impairs mappability and an alternative approach like hash-based enumeration approaches or asymmetric search paths during a BWT-based backward search might prove superior. Beyond evaluating mapping approaches transcriptome-wide, we demonstrate how \textit{splice\_sim} can be used to evaluate alternative sequencing approaches by simulating 3'end datasets and benchmarking them against the conventional full-length transcript sequencing approach. We find that 3'end sequencing compares well with full-length transcript sequencing across most mappability categories when comparing biological readouts such as the fraction of converted reads, thus providing a valid, cost-effective alternative. In addition, we could demonstrate how parametrization such as supplying known splice-sites to the mapping process can severely impact mapping performance and the biases of the resulting analysis. \\
Limitations of \textit{splice\_sim} include firstly limitations to any simulation approach: Since it is for obvious reasons impossible to enumerate and analytically iterate over all possible nucleotide conversion read states, resulting read sets potentially contain biases due to stochasticity. We tried to assess this effect by reproducing results in several simulated replicates which displayed high correlation, therefore providing additional confidence that such stochastic biases have little impact on the interpretation. A limitation of the transcriptome-wide simulation where we chose one canonical transcript variant per gene and simulate reads with comparable coverage is that this does not represent a true biological scenario, where only a subset of genes is expressed. Such simulated read sets have the highest potential for mismapping and false-positive read mappings and therefore represent worst case scenarios. In practice, it is rather recommended to first subselect genes based on expression levels in a cell-type of interest to mitigate this bias. Similar to SLAM-DUNK, \textit{splice\_sim} currently only supports simulation of single-end reads which is a commonly used sequencing mode but slowly on the decline and arguably paired-end reads would improve mappability and also on top allow for sequencing error correction in the overlapping read parts. This caveat could be addressed by the modular design of \textit{splice\_sim}, which allows for a different parametrization and exchange of the underlying simulation to in the future support paired-end read simulation. Currently, \textit{splice\_sim} also does not allow for the simulation of long reads such as produced by the PacBio or Nanopore sequencing platform which is of increasing interest in the field. However, again via the modular design, one could in a relatively straightforward manner exchange the underlying simulation engine to e.g. PBSIM to enable long read simulation support. \\
\textit{Splice\_sim} is supplied as parallelized Nextflow workflow with all dependencies wrapped in a Docker container and therefore readily usable for the scientific community with minimal burden of installation. We supply transcriptome wide mappability tracks for the human and mouse transcriptome, but given a reference sequence and genome annotation in canonical \texttt{fasta} and \texttt{gff3} formats as only prerequisite, scientists are encouraged and enabled to freely evaluate alternative organisms, references, nucleotide conversion types and rates, sequencing techniques, parameters and mapping tools. Complemented with our DUNK method and SLAM-DUNK implementation, we have provided the scientific community with a first means of robustly quantifying nucleotide conversions in epitranscriptomics sequencing datasets and evaluating the accuracy and impact of the produced results on the final biological readout and interpretation.